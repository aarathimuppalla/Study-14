{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import openml as oml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot\n",
    "import sklearn.tree\n",
    "import sklearn.preprocessing\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\n",
    "from openmlstudy14.preprocessing import ConditionalImputer\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 1: Get datasets from OpenML\n",
    "- Only classification datasets\n",
    "- Only active (verified) datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1038 active classification datasets\n"
     ]
    }
   ],
   "source": [
    "# Get all OpenML datasets\n",
    "status_type = 'active'\n",
    "openml_list = oml.datasets.list_datasets(status=status_type) # Returns a dict\n",
    "datalist = pd.DataFrame.from_dict(openml_list, orient='index') # Transform to pandas\n",
    "#datalist = datalist[datalist.status == 'active'] # Only use active (verified) datasets\n",
    "datalist = datalist[datalist.NumberOfClasses>=2] # Only classification\n",
    "print(\"{} {} classification datasets\".format(len(datalist), status_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Bookkeeping\n",
    "data_names = {k: v for (k, v) in datalist[['did','name']].values} # dataset names\n",
    "data_status = {k: 'OK' for k in datalist.index} # dataset status (OK or reason for removal)\n",
    "datalist_full = datalist.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 1: Apply simple preconditions\n",
    "- Number of observations larger than 500 (meaningful evaluations)\n",
    "- Number of observations smaller than 100000 (keep runtime manageable)\n",
    "- Number of features does not exceed 5000 (keep runtime manageable)\n",
    "- The ratio of the minority class and the majority class is > 0.05 (severely imbalanced datasets complicate analysis)\n",
    "- Number of values for categorical features must not exceed 100 (severely slows down some algorithms)\n",
    "- Sparsely formatted data (requires special data readers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Too small', 340],\n",
       " ['OK', 311],\n",
       " ['Sparse format', 32],\n",
       " ['High-dimensional', 65],\n",
       " ['Too large', 110],\n",
       " ['Extreme imbalance', 180]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply preconditions\n",
    "data_status.update({k: 'Too small' for k in datalist.index[datalist.NumberOfInstances<500]})\n",
    "data_status.update({k: 'Too large' for k in datalist.index[datalist.NumberOfInstances>100000]})\n",
    "data_status.update({k: 'High-dimensional' for k in datalist.index[datalist.NumberOfFeatures>2000]})\n",
    "data_status.update({k: 'Extreme imbalance' for k in datalist.index[datalist.MinorityClassSize / datalist.MajorityClassSize < 0.05]})\n",
    "#data_status.update({k: 'Too many categories' for k in datalist.index[datalist.MaxNominalAttDistinctValues > 100]})\n",
    "data_status.update({k: 'Sparse format' for k in datalist.index[datalist.format == 'Sparse_ARFF']})\n",
    "\n",
    "# Filter dataset list\n",
    "datalist = datalist[pd.Series({k:(v=='OK') for k,v in data_status.items()})] \n",
    "\n",
    "# Status update\n",
    "[[x,list(data_status.values()).count(x)] for x in set(data_status.values())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 2: Filter out special datasets\n",
    "- Artificial datasets (may bias the results)\n",
    "- Time series dataset (cannot use random sampling for evaluation)\n",
    "- Text data (contains string features which need additional preprocessing)\n",
    "- Multilabel data (multiple targets need to be predicted)\n",
    "- Derived versions of datasets (with additional preprocessing)\n",
    "- Datasets where the intended classification target is unclear\n",
    "- Binarized regression problems\n",
    "- Unknown origin (no description how data was collected and what the problem is)\n",
    "- Grouped data (instances form groups (blocks) and can't be randomly sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vanrijn/projects/pythonvirtual/pimp/lib/python3.5/site-packages/ipykernel_launcher.py:24: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Binarized regression problem', 88],\n",
       " ['Unknown origin', 8],\n",
       " ['Time series data', 6],\n",
       " ['Too small', 339],\n",
       " ['Artificial data', 203],\n",
       " ['Text data', 2],\n",
       " ['Multi-label data', 7],\n",
       " ['OK', 129],\n",
       " ['Sparse format', 32],\n",
       " ['High-dimensional', 64],\n",
       " ['Unspecified target feature', 5],\n",
       " ['Too large', 23],\n",
       " ['Grouped data', 2],\n",
       " ['Derived (non-original) data', 40],\n",
       " ['Extreme imbalance', 113]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get lists of special datasets\n",
    "artificial_set = set(oml.datasets.list_datasets(tag=\"artificial\").keys())\n",
    "timeseries_set = set(oml.datasets.list_datasets(tag=\"time_series\").keys())\n",
    "text_set = set(oml.datasets.list_datasets(tag=\"text_data\").keys())\n",
    "multilabel_set = set(oml.datasets.list_datasets(tag=\"multi_label\").keys())\n",
    "derived_set = set(oml.datasets.list_datasets(tag=\"derived\").keys())\n",
    "unspecified_set = set(oml.datasets.list_datasets(tag=\"unspecified_target_feature\").keys())\n",
    "binarized_set = set(oml.datasets.list_datasets(tag=\"binarized_regression_problem\").keys())\n",
    "unknown_set = set(oml.datasets.list_datasets(tag=\"origin_unknown\").keys())\n",
    "grouped_set = set(oml.datasets.list_datasets(tag=\"grouped_data\").keys())\n",
    "\n",
    "data_status.update({k: 'Artificial data' for k in artificial_set})\n",
    "data_status.update({k: 'Time series data' for k in timeseries_set})\n",
    "data_status.update({k: 'Text data' for k in text_set})\n",
    "data_status.update({k: 'Multi-label data' for k in multilabel_set})\n",
    "data_status.update({k: 'Derived (non-original) data' for k in derived_set})\n",
    "data_status.update({k: 'Unspecified target feature' for k in unspecified_set})\n",
    "data_status.update({k: 'Binarized regression problem' for k in binarized_set})\n",
    "data_status.update({k: 'Unknown origin' for k in unknown_set})\n",
    "data_status.update({k: 'Grouped data' for k in grouped_set})\n",
    "#data_status.update({k: 'OpenML100' for k in openml100_set})\n",
    "\n",
    "# Filter dataset list\n",
    "datalist = datalist[pd.Series({k:(v=='OK') for k,v in data_status.items()})] \n",
    "\n",
    "# Status update\n",
    "[[x,list(data_status.values()).count(x)] for x in set(data_status.values())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 3: Remove alternative versions of datasets\n",
    "- Remove binarized versions of multi-class datasets\n",
    "- Check other possible duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vanrijn/projects/pythonvirtual/pimp/lib/python3.5/site-packages/ipykernel_launcher.py:43: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Text data', 2],\n",
       " ['Binarized version of multiclass dataset', 77],\n",
       " ['OK', 91],\n",
       " ['Grouped data', 2],\n",
       " ['Extreme imbalance', 112],\n",
       " ['Duplicate of 1590', 1],\n",
       " ['Multi-label data', 6],\n",
       " ['Sparse format', 31],\n",
       " ['Duplicate of 40983', 1],\n",
       " ['High-dimensional', 64],\n",
       " ['Unspecified target feature', 5],\n",
       " ['Duplicate of 40979', 2],\n",
       " ['Duplicate of 40984', 2],\n",
       " ['Unknown origin', 8],\n",
       " ['Duplicate of 772', 1],\n",
       " ['Too small', 298],\n",
       " ['Artificial data', 202],\n",
       " ['Binarized regression problem', 86],\n",
       " ['Time series data', 6],\n",
       " ['Duplicate of 40994', 1],\n",
       " ['Duplicate of 40982', 1],\n",
       " ['Too large', 23],\n",
       " ['Duplicate of 40945', 1],\n",
       " ['Derived (non-original) data', 38]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sorting makes things easier\n",
    "# We need the full list because there may be binarized versions of already removed datasets\n",
    "datalist_full = datalist_full.sort_values(by=['name','NumberOfClasses'], ascending=[True, False])\n",
    "\n",
    "checked_datasets = {\n",
    "    40979: [1022, 20], # Correct version of mfeat-pixel\n",
    "    40984: [958, 36], # Correct version of segment\n",
    "    40994: [40990, 40989, 1467], # Correct version of climate-model-simulation-crashes\n",
    "    1590: [179], # Correct version of adult\n",
    "    40983: [1570], # Correct version of wilt\n",
    "    40945: [40704], # Correct version of Titanic\n",
    "    772: [948], # Correct version of classification version of the quake dataset\n",
    "    40966: [40965, 40964], # Correct version of MiceProtein (needed when including in_preparation datasets)\n",
    "    40982: [40973, 1504], # Correct version of steel-plates-fault (needed when including in_preparation datasets)\n",
    "    23380: [1024, 473], # Correct version of cjs (needed when including in_preparation datasets)\n",
    "    40597 : [40733], # Correct version of yeast (needed when including in_preparation datasets)\n",
    "    1046 : [40829], # Correct version of mozilla4 (needed when including in_preparation datasets)\n",
    "    # : [40958], # Correct version of Bankdata (needed when including in_preparation datasets)\n",
    "    \n",
    "}\n",
    "duplicates_of = {}\n",
    "# Mark the duplicates of datasets where we know which version is the correct one!\n",
    "for cd in checked_datasets:\n",
    "    for dup_id in checked_datasets[cd]:\n",
    "        duplicates_of[dup_id] = cd\n",
    "\n",
    "data_unique = {}\n",
    "for index, row in datalist_full.iterrows():\n",
    "    if row['did'] in duplicates_of:\n",
    "        data_status[row['did']] = 'Duplicate of %d' % duplicates_of[row['did']]\n",
    "    elif row['did'] in checked_datasets and data_status[row['did']] in ('OK', 'Possible duplicate'):\n",
    "        data_status[row['did']] = 'OK'\n",
    "    elif row['name'] not in data_unique:\n",
    "        data_unique[row['name']] = row\n",
    "    else:\n",
    "        previous = data_unique[row['name']]\n",
    "        if previous['NumberOfClasses'] > 2 and row['NumberOfClasses'] == 2:\n",
    "            data_status[row['did']] = 'Binarized version of multiclass dataset'\n",
    "        elif data_status[row['did']] in ('OK', 'Possible duplicate'):\n",
    "            data_status[row['did']] = 'Possible duplicate'\n",
    "\n",
    "# Filter dataset list\n",
    "datalist = datalist[pd.Series({k:(v=='OK') for k,v in data_status.items()})] \n",
    "               \n",
    "# Status update\n",
    "[[x,list(data_status.values()).count(x)] for x in set(data_status.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These need to be checked\n",
    "[k for k,v in data_status.items() if v=='Possible duplicate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 4: Remove trivial datasets\n",
    "- See if a model (e.g. random forest) based on 1 feature can get perfect CV performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing dataset 3 kr-vs-kp ( 1 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 6 letter ( 2 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 11 balance-scale ( 3 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 12 mfeat-factors ( 4 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 14 mfeat-fourier ( 5 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 15 breast-w ( 6 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 16 mfeat-karhunen ( 7 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 18 mfeat-morphological ( 8 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40979 mfeat-pixel ( 9 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40981 Australian ( 10 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 22 mfeat-zernike ( 11 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 23 cmc ( 12 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 24 mushroom ( 13 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 28 optdigits ( 14 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 29 credit-approval ( 15 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 31 credit-g ( 16 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 32 pendigits ( 17 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 37 diabetes ( 18 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 38 sick ( 19 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 42 soybean ( 20 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 44 spambase ( 21 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 46 splice ( 22 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 50 tic-tac-toe ( 23 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 54 vehicle ( 24 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 60 waveform-5000 ( 25 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40971 collins ( 26 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40978 Internet-Advertisements ( 27 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "40978 sparse matrix length is ambiguous; use getnnz() or shape[0]\n",
      "processing dataset 40984 segment ( 28 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 151 electricity ( 29 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 182 satimage ( 30 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 188 eucalyptus ( 31 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40994 climate-model-simulation-crashes ( 32 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40996 Fashion-MNIST ( 33 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 4134 Bioresponse ( 34 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 4135 Amazon_employee_access ( 35 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "4135 sparse matrix length is ambiguous; use getnnz() or shape[0]\n",
      "processing dataset 300 isolet ( 36 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 307 vowel ( 37 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 377 synthetic_control ( 38 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 41027 jungle_chess_2pcs_raw_endgame_complete ( 39 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 4534 PhishingWebsites ( 40 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 4538 GesturePhaseSegmentationProcessed ( 41 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 451 irish ( 42 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 469 analcatdata_dmft ( 43 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 554 mnist_784 ( 44 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1038 gina_agnostic ( 45 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1049 pc4 ( 46 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1050 pc3 ( 47 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1053 jm1 ( 48 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1063 kc2 ( 49 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1067 kc1 ( 50 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1068 pc1 ( 51 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1112 KDDCup09_churn ( 52 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "1112 sparse matrix length is ambiguous; use getnnz() or shape[0]\n",
      "processing dataset 1114 KDDCup09_upselling ( 53 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "1114 sparse matrix length is ambiguous; use getnnz() or shape[0]\n",
      "processing dataset 1116 musk ( 54 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "Dataset  musk is too easy.\n",
      "processing dataset 1120 MagicTelescope ( 55 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40966 MiceProtein ( 56 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1461 bank-marketing ( 57 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1462 banknote-authentication ( 58 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1464 blood-transfusion-service-center ( 59 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1466 cardiotocography ( 60 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1468 cnae-9 ( 61 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1475 first-order-theorem-proving ( 62 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1478 har ( 63 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1480 ilpd ( 64 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1485 madelon ( 65 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1486 nomao ( 66 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1487 ozone-level-8hr ( 67 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1489 phoneme ( 68 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1491 one-hundred-plants-margin ( 69 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1492 one-hundred-plants-shape ( 70 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1493 one-hundred-plants-texture ( 71 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1494 qsar-biodeg ( 72 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1497 wall-robot-navigation ( 73 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1501 semeion ( 74 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1510 wdbc ( 75 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1515 micro-mass ( 76 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40499 texture ( 77 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 1590 adult ( 78 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40536 SpeedDating ( 79 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 23380 cjs ( 80 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 6332 cylinder-bands ( 81 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 23381 dresses-sales ( 82 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40668 connect-4 ( 83 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40670 dna ( 84 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40701 churn ( 85 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40705 tokyo1 ( 86 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 23512 higgs ( 87 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 23517 numerai28.6 ( 88 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40982 steel-plates-fault ( 89 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40983 wilt ( 90 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40923 Devnagari-Script ( 91 / 91 )\n",
      "hotencoding dataset ... (for too much features)\n",
      "building decision stump\n"
     ]
    }
   ],
   "source": [
    "datasets = [k for k,v in data_status.items() if v=='OK']\n",
    "\n",
    "max_score_per_dataset = {}\n",
    "for idx, dataset_id in enumerate(datasets):\n",
    "    try:\n",
    "        \n",
    "        dataset = oml.datasets.get_dataset(dataset_id)\n",
    "        print('processing dataset', dataset_id, dataset.name, '(',idx+1, '/', len(datasets), ')')\n",
    "        if dataset.default_target_attribute is None:\n",
    "            data_status[dataset_id] = 'No target specified'\n",
    "            print('No target')\n",
    "            continue\n",
    "        X, y = dataset.get_data(target=dataset.default_target_attribute)\n",
    "\n",
    "        print('hotencoding dataset ... (for too much features)')\n",
    "        categorical_indices = dataset.get_features_by_type('nominal', [dataset.default_target_attribute])\n",
    "        clf = sklearn.pipeline.Pipeline(steps=[('imputer', ConditionalImputer(strategy='median', strategy_nominal='mean', categorical_features=categorical_indices, fill_empty=-1)), \n",
    "                                               ('encoder', sklearn.preprocessing.OneHotEncoder(categorical_features=categorical_indices))])\n",
    "        hotencoded = clf.fit_transform(X)\n",
    "        if hotencoded.shape[1] > 2000:\n",
    "            data_status[dataset_id] = 'hotencoded-dimensionality'\n",
    "            print(dataset.name, 'too high hot-encoded dimensionality', len(hotencoded))\n",
    "            continue\n",
    "\n",
    "        print('building decision stump')\n",
    "        clf = sklearn.pipeline.Pipeline(steps=[('imputer', sklearn.preprocessing.Imputer(strategy='median')), \n",
    "                                               ('classifier', sklearn.tree.DecisionTreeClassifier(max_depth=1))])\n",
    "        _ = clf.fit(X, y)\n",
    "        score = clf.score(X, y)\n",
    "        \n",
    "        print('obtaining cv task .. ')\n",
    "        # TODO\n",
    "\n",
    "        max_score_per_dataset[dataset_id] = {\n",
    "            'score': score,\n",
    "            'name': dataset.name\n",
    "        }\n",
    "    except ValueError as e:\n",
    "        data_status[dataset_id] = 'Python ValueError'\n",
    "        print(dataset_id, e)\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        data_status[dataset_id] = 'Python Exception'\n",
    "        print(dataset_id, e)\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    if max_score_per_dataset[dataset_id][\"score\"] == 1.00:\n",
    "        data_status[dataset_id] = 'Too easy'\n",
    "        print(\"Dataset \", dataset.name, \"is too easy.\")\n",
    "    \n",
    "results = pd.DataFrame(max_score_per_dataset).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results.sort_values(by='score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Results\n",
    "Final list of selected datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "final_datasets = [k for k,v in data_status.items() if v=='OK']\n",
    "print('{} datasets selected'.format(len(final_datasets)))\n",
    "{k:v for k,v in data_names.items() if k in final_datasets}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Passed all tests, but not in OpenML100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "openml100_set = set(oml.datasets.list_datasets(tag=\"OpenML100\").keys()) # OpenML100\n",
    "\n",
    "new_datasets = [k for k,v in data_status.items() if v=='OK' and k not in openml100_set]\n",
    "{k:v for k,v in data_names.items() if k in new_datasets}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Datasets tagged with OpenML100 that did not pass all tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_datasets = [k for k,v in data_status.items() if k in openml100_set and v!='OK']\n",
    "{k:v for k,v in data_names.items() if k in new_datasets}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Reasons to exclude datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "v = {}\n",
    "for key, value in sorted(data_status.items()):\n",
    "    v.setdefault(value, []).append(key)\n",
    "{k:str(v) for k,v in v.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Difference to google doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "names_selected = {v for k,v in data_names.items() if k in final_datasets}\n",
    "google_doc = {\n",
    "    \"phoneme\",\"breast-w\",\"Australian\",\"banknote-authentication\",\"eeg-eye-state\",\"electricity\",\n",
    "    \"analcatdata_dmft\",\"higgs\",\"blood-transfusion-service-center\",\"ilpd\",\"steel-plates-fault\",\n",
    "    \"satimage\",\"mfeat-morphological\",\"balance-scale\",\"credit-a\",\"cmc\",\"diabetes\",\"wilt\",\"MagicTelescope\",\n",
    "    \"vowel\",\"pc3\",\"adult\",\"pc4\",\"ada_agnostic\",\"GesturePhaseSegmentationProcessed\",\"PhishingWebsites\",\n",
    "    \"bank-marketing\",\"cardiotocography\",\"climate-model-simulation-crashes\",\"first-order-theorem-proving\",\n",
    "    \"wall-robot-navigation\",\"dresses-sales\",\"sick\",\"waveform-5000\",\"wdbc\",\"car\",\"tic-tac-toe\",\"mfeat-zernike\",\n",
    "    \"segment\",\"connect-4\",\"kc2\",\"jm1\",\"pc1\",\"kc1\",\"qsar-biodeg\",\"eucalyptus\",\"credit-g\",\"pendigits\",\"vehicle\",\n",
    "    \"letter\",\"optdigits\",\"mfeat-fourier\",\"mfeat-karhunen\",\"kr-vs-kp\",\"ozone-level-8hr\",\"sylva_agnostic\",\n",
    "    \"nomao\",\"spambase\",\"splice\",\"mushroom\",\"cylinder-bands\",\"SpeedDating\",\"texture\",\"mfeat-factors\",\"collins\",\n",
    "    \"mnist_784\",\"gina_agnostic\",\"Bioresponse\",\"Internet-Advertisements\",\"semeion\",\"soybean\",\"madelon\",\"har\",\n",
    "    \"isolet\",\"micro-mass\",\"cnae-9\",\"MiceProtein\",\"one-hundred-plants-margin\",\"one-hundred-plants-shape\",\n",
    "    \"one-hundred-plants-texture\",\"mfeat-pixel\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"New ones!\")\n",
    "for d in sorted(names_selected - google_doc):\n",
    "    print('  ', d)\n",
    "print(\"Dropped ones!\")\n",
    "for d in sorted(google_doc - names_selected):\n",
    "    print('  ', d)\n",
    "print('Difference')\n",
    "for d in sorted(google_doc ^ names_selected):\n",
    "    print('  ', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (pimp)",
   "language": "python",
   "name": "pimp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
