{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import openml as oml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot\n",
    "import sklearn.tree\n",
    "import sklearn.preprocessing\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\n",
    "from openmlstudy14.preprocessing import ConditionalImputer\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 1: Get datasets from OpenML\n",
    "- Only classification datasets\n",
    "- Only active (verified) datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1038 active classification datasets\n"
     ]
    }
   ],
   "source": [
    "# Get all OpenML datasets\n",
    "status_type = 'active'\n",
    "openml_list = oml.datasets.list_datasets(status=status_type) # Returns a dict\n",
    "datalist = pd.DataFrame.from_dict(openml_list, orient='index') # Transform to pandas\n",
    "#datalist = datalist[datalist.status == 'active'] # Only use active (verified) datasets\n",
    "datalist = datalist[datalist.NumberOfClasses>=2] # Only classification\n",
    "print(\"{} {} classification datasets\".format(len(datalist), status_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Bookkeeping\n",
    "data_names = {k: v for (k, v) in datalist[['did','name']].values} # dataset names\n",
    "data_status = {k: 'OK' for k in datalist.index} # dataset status (OK or reason for removal)\n",
    "datalist_full = datalist.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 1: Apply simple preconditions\n",
    "- Number of observations larger than 500 (meaningful evaluations)\n",
    "- Number of observations smaller than 100000 (keep runtime manageable)\n",
    "- Number of features does not exceed 5000 (keep runtime manageable)\n",
    "- The ratio of the minority class and the majority class is > 0.05 (severely imbalanced datasets complicate analysis)\n",
    "- Number of values for categorical features must not exceed 100 (severely slows down some algorithms)\n",
    "- Sparsely formatted data (requires special data readers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Sparse format', 32],\n",
       " ['Extreme imbalance', 180],\n",
       " ['Too small', 343],\n",
       " ['OK', 313],\n",
       " ['High-dimensional', 60],\n",
       " ['Too large', 110]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply preconditions\n",
    "data_status.update({k: 'Too small' for k in datalist.index[datalist.NumberOfInstances<500]})\n",
    "data_status.update({k: 'Too large' for k in datalist.index[datalist.NumberOfInstances>100000]})\n",
    "data_status.update({k: 'High-dimensional' for k in datalist.index[datalist.NumberOfFeatures>5000]})\n",
    "data_status.update({k: 'Extreme imbalance' for k in datalist.index[datalist.MinorityClassSize / datalist.MajorityClassSize < 0.05]})\n",
    "#data_status.update({k: 'Too many categories' for k in datalist.index[datalist.MaxNominalAttDistinctValues > 100]})\n",
    "data_status.update({k: 'Sparse format' for k in datalist.index[datalist.format == 'Sparse_ARFF']})\n",
    "\n",
    "# Filter dataset list\n",
    "datalist = datalist[pd.Series({k:(v=='OK') for k,v in data_status.items()})] \n",
    "\n",
    "# Status update\n",
    "[[x,list(data_status.values()).count(x)] for x in set(data_status.values())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 2: Filter out special datasets\n",
    "- Artificial datasets (may bias the results)\n",
    "- Time series dataset (cannot use random sampling for evaluation)\n",
    "- Text data (contains string features which need additional preprocessing)\n",
    "- Multilabel data (multiple targets need to be predicted)\n",
    "- Derived versions of datasets (with additional preprocessing)\n",
    "- Datasets where the intended classification target is unclear\n",
    "- Binarized regression problems\n",
    "- Unknown origin (no description how data was collected and what the problem is)\n",
    "- Grouped data (instances form groups (blocks) and can't be randomly sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vanrijn/projects/pythonvirtual/pimp/lib/python3.5/site-packages/ipykernel_launcher.py:26: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Text data', 2],\n",
       " ['Time series data', 7],\n",
       " ['Sparse format', 32],\n",
       " ['Label leakage', 2],\n",
       " ['Extreme imbalance', 113],\n",
       " ['Too small', 342],\n",
       " ['Derived (non-original) data', 40],\n",
       " ['Binarized regression problem', 88],\n",
       " ['OK', 127],\n",
       " ['Artificial data', 203],\n",
       " ['High-dimensional', 60],\n",
       " ['Unspecified target feature', 5],\n",
       " ['Unknown origin', 8],\n",
       " ['Too large', 23],\n",
       " ['Grouped data', 2],\n",
       " ['Multi-label data', 7]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get lists of special datasets\n",
    "artificial_set = set(oml.datasets.list_datasets(tag=\"artificial\", status=status_type).keys())\n",
    "timeseries_set = set(oml.datasets.list_datasets(tag=\"time_series\", status=status_type).keys())\n",
    "text_set = set(oml.datasets.list_datasets(tag=\"text_data\", status=status_type).keys())\n",
    "multilabel_set = set(oml.datasets.list_datasets(tag=\"multi_label\", status=status_type).keys())\n",
    "derived_set = set(oml.datasets.list_datasets(tag=\"derived\", status=status_type).keys())\n",
    "unspecified_set = set(oml.datasets.list_datasets(tag=\"unspecified_target_feature\", status=status_type).keys())\n",
    "binarized_set = set(oml.datasets.list_datasets(tag=\"binarized_regression_problem\", status=status_type).keys())\n",
    "unknown_set = set(oml.datasets.list_datasets(tag=\"origin_unknown\", status=status_type).keys())\n",
    "grouped_set = set(oml.datasets.list_datasets(tag=\"grouped_data\", status=status_type).keys())\n",
    "label_leakage = set(oml.datasets.list_datasets(tag=\"label_leakage\", status=status_type).keys())\n",
    "\n",
    "data_status.update({k: 'Artificial data' for k in artificial_set})\n",
    "data_status.update({k: 'Time series data' for k in timeseries_set})\n",
    "data_status.update({k: 'Text data' for k in text_set})\n",
    "data_status.update({k: 'Multi-label data' for k in multilabel_set})\n",
    "data_status.update({k: 'Derived (non-original) data' for k in derived_set})\n",
    "data_status.update({k: 'Unspecified target feature' for k in unspecified_set})\n",
    "data_status.update({k: 'Binarized regression problem' for k in binarized_set})\n",
    "data_status.update({k: 'Unknown origin' for k in unknown_set})\n",
    "data_status.update({k: 'Grouped data' for k in grouped_set})\n",
    "data_status.update({k: 'Label leakage' for k in label_leakage})\n",
    "#data_status.update({k: 'OpenML100' for k in openml100_set})\n",
    "\n",
    "# Filter dataset list\n",
    "datalist = datalist[pd.Series({k:(v=='OK') for k,v in data_status.items()})] \n",
    "\n",
    "# Status update\n",
    "[[x,list(data_status.values()).count(x)] for x in set(data_status.values())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 3: Remove alternative versions of datasets\n",
    "- Remove binarized versions of multi-class datasets\n",
    "- Check other possible duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vanrijn/projects/pythonvirtual/pimp/lib/python3.5/site-packages/ipykernel_launcher.py:43: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Time series data', 7],\n",
       " ['Binarized version of multiclass dataset', 77],\n",
       " ['Too small', 301],\n",
       " ['Grouped data', 2],\n",
       " ['Text data', 2],\n",
       " ['Label leakage', 2],\n",
       " ['Derived (non-original) data', 38],\n",
       " ['OK', 89],\n",
       " ['Duplicate of 772', 1],\n",
       " ['Duplicate of 1590', 1],\n",
       " ['Unspecified target feature', 5],\n",
       " ['Multi-label data', 6],\n",
       " ['Duplicate of 40984', 2],\n",
       " ['Duplicate of 40994', 1],\n",
       " ['Extreme imbalance', 112],\n",
       " ['High-dimensional', 60],\n",
       " ['Too large', 23],\n",
       " ['Sparse format', 31],\n",
       " ['Duplicate of 40979', 2],\n",
       " ['Duplicate of 40982', 1],\n",
       " ['Duplicate of 40945', 1],\n",
       " ['Binarized regression problem', 86],\n",
       " ['Artificial data', 202],\n",
       " ['Duplicate of 40983', 1],\n",
       " ['Unknown origin', 8]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sorting makes things easier\n",
    "# We need the full list because there may be binarized versions of already removed datasets\n",
    "datalist_full = datalist_full.sort_values(by=['name','NumberOfClasses'], ascending=[True, False])\n",
    "\n",
    "checked_datasets = {\n",
    "    40979: [1022, 20], # Correct version of mfeat-pixel\n",
    "    40984: [958, 36], # Correct version of segment\n",
    "    40994: [40990, 40989, 1467], # Correct version of climate-model-simulation-crashes\n",
    "    1590: [179], # Correct version of adult\n",
    "    40983: [1570], # Correct version of wilt\n",
    "    40945: [40704], # Correct version of Titanic\n",
    "    772: [948], # Correct version of classification version of the quake dataset\n",
    "    40966: [40965, 40964], # Correct version of MiceProtein (needed when including in_preparation datasets)\n",
    "    40982: [40973, 1504], # Correct version of steel-plates-fault (needed when including in_preparation datasets)\n",
    "    23380: [1024, 473], # Correct version of cjs (needed when including in_preparation datasets)\n",
    "    40597 : [40733], # Correct version of yeast (needed when including in_preparation datasets)\n",
    "    1046 : [40829], # Correct version of mozilla4 (needed when including in_preparation datasets)\n",
    "    # : [40958], # Correct version of Bankdata (needed when including in_preparation datasets)\n",
    "    \n",
    "}\n",
    "duplicates_of = {}\n",
    "# Mark the duplicates of datasets where we know which version is the correct one!\n",
    "for cd in checked_datasets:\n",
    "    for dup_id in checked_datasets[cd]:\n",
    "        duplicates_of[dup_id] = cd\n",
    "\n",
    "data_unique = {}\n",
    "for index, row in datalist_full.iterrows():\n",
    "    if row['did'] in duplicates_of:\n",
    "        data_status[row['did']] = 'Duplicate of %d' % duplicates_of[row['did']]\n",
    "    elif row['did'] in checked_datasets and data_status[row['did']] in ('OK', 'Possible duplicate'):\n",
    "        data_status[row['did']] = 'OK'\n",
    "    elif row['name'] not in data_unique:\n",
    "        data_unique[row['name']] = row\n",
    "    else:\n",
    "        previous = data_unique[row['name']]\n",
    "        if previous['NumberOfClasses'] > 2 and row['NumberOfClasses'] == 2:\n",
    "            data_status[row['did']] = 'Binarized version of multiclass dataset'\n",
    "        elif data_status[row['did']] in ('OK', 'Possible duplicate'):\n",
    "            data_status[row['did']] = 'Possible duplicate'\n",
    "\n",
    "# Filter dataset list\n",
    "datalist = datalist[pd.Series({k:(v=='OK') for k,v in data_status.items()})] \n",
    "               \n",
    "# Status update\n",
    "[[x,list(data_status.values()).count(x)] for x in set(data_status.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These need to be checked\n",
    "[k for k,v in data_status.items() if v=='Possible duplicate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 4: Remove trivial datasets\n",
    "- See if a model (e.g. random forest) based on 1 feature can get perfect CV performance (JvR, removed this code. But Irish and cjs came out of this check. I will tag them. )\n",
    "- See if a CV Decision Tree (flow id 7777) or a CV logistic regression (flow id 7778) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Time series data', 7],\n",
       " ['Binarized version of multiclass dataset', 77],\n",
       " ['Too easy (perfect score decision tree)', 2],\n",
       " ['Too small', 301],\n",
       " ['Dataset not checked for triviality by DT!', 3],\n",
       " ['Grouped data', 2],\n",
       " ['Text data', 2],\n",
       " ['Label leakage', 2],\n",
       " ['Derived (non-original) data', 38],\n",
       " ['OK', 84],\n",
       " ['Duplicate of 772', 1],\n",
       " ['Duplicate of 1590', 1],\n",
       " ['Unspecified target feature', 5],\n",
       " ['Multi-label data', 6],\n",
       " ['Duplicate of 40984', 2],\n",
       " ['Duplicate of 40994', 1],\n",
       " ['Extreme imbalance', 112],\n",
       " ['High-dimensional', 60],\n",
       " ['Too large', 23],\n",
       " ['Sparse format', 31],\n",
       " ['Duplicate of 40979', 2],\n",
       " ['Duplicate of 40982', 1],\n",
       " ['Duplicate of 40945', 1],\n",
       " ['Binarized regression problem', 86],\n",
       " ['Artificial data', 202],\n",
       " ['Duplicate of 40983', 1],\n",
       " ['Unknown origin', 8]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perfect_scores_dt = {int(v.data_id) for k, v in oml.evaluations.list_evaluations('predictive_accuracy', flow=[7777]).items() if v.value == 1}\n",
    "perfect_scores_lr = {int(v.data_id) for k, v in oml.evaluations.list_evaluations('predictive_accuracy', flow=[7778]).items() if v.value == 1}\n",
    "\n",
    "for did in perfect_scores_dt:\n",
    "    if data_status[did] == 'OK':\n",
    "        data_status[did] = 'Too easy (perfect score decision tree)'\n",
    "        \n",
    "for did in perfect_scores_lr:\n",
    "    if data_status[did] == 'OK':\n",
    "        data_status[did] = 'Too easy (perfect score logistic regression)'\n",
    "     \n",
    "checked_datasets = {int(v.data_id) for v in oml.evaluations.list_evaluations('predictive_accuracy', flow=[7777]).values()}\n",
    "for did in data_status:\n",
    "    if data_status[did] == 'OK' and did not in checked_datasets:\n",
    "        data_status[did] = 'Dataset not checked for triviality by DT!'\n",
    "checked_datasets = {int(v.data_id) for v in oml.evaluations.list_evaluations('predictive_accuracy', flow=[7778]).values()}\n",
    "# for did in data_status:\n",
    "#     if data_status[did] == 'OK' and did not in checked_datasets:\n",
    "#         data_status[did] = 'Dataset not checked for triviality by LR!'\n",
    "\n",
    "[[x,list(data_status.values()).count(x)] for x in set(data_status.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing dataset 3 kr-vs-kp ( 1 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 6 letter ( 2 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 11 balance-scale ( 3 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 12 mfeat-factors ( 4 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 14 mfeat-fourier ( 5 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 15 breast-w ( 6 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 16 mfeat-karhunen ( 7 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 18 mfeat-morphological ( 8 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40979 mfeat-pixel ( 9 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40981 Australian ( 10 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 22 mfeat-zernike ( 11 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 23 cmc ( 12 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 28 optdigits ( 13 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 29 credit-approval ( 14 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 31 credit-g ( 15 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 32 pendigits ( 16 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 37 diabetes ( 17 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 38 sick ( 18 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 42 soybean ( 19 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 44 spambase ( 20 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 46 splice ( 21 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 50 tic-tac-toe ( 22 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 54 vehicle ( 23 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 60 waveform-5000 ( 24 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40971 collins ( 25 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40984 segment ( 26 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 151 electricity ( 27 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 182 satimage ( 28 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 188 eucalyptus ( 29 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40994 climate-model-simulation-crashes ( 30 / 84 )\n",
      "One hot encoding dataset to check for the true amount of total features.\n",
      "building decision stump\n",
      "obtaining cv task .. \n",
      "processing dataset 40996 Fashion-MNIST ( 31 / 84 )\n"
     ]
    }
   ],
   "source": [
    "datasets = [k for k,v in data_status.items() if v=='OK']\n",
    "\n",
    "max_score_per_dataset = {}\n",
    "for idx, dataset_id in enumerate(datasets):\n",
    "    try:\n",
    "        \n",
    "        dataset = oml.datasets.get_dataset(dataset_id)\n",
    "        print('processing dataset', dataset_id, dataset.name, '(',idx+1, '/', len(datasets), ')')\n",
    "        if dataset.default_target_attribute is None:\n",
    "            data_status[dataset_id] = 'No target specified'\n",
    "            print('No target')\n",
    "            continue\n",
    "        X, y = dataset.get_data(target=dataset.default_target_attribute)\n",
    "\n",
    "        print('One hot encoding dataset to check for the true amount of total features.')\n",
    "        categorical_indices = dataset.get_features_by_type('nominal', [dataset.default_target_attribute])\n",
    "        clf = sklearn.pipeline.Pipeline(\n",
    "            steps=[\n",
    "                (\n",
    "                    'imputer', ConditionalImputer(\n",
    "                        strategy='median', \n",
    "                        strategy_nominal='mean', \n",
    "                        categorical_features=categorical_indices, \n",
    "                        fill_empty=-1,\n",
    "                    )\n",
    "                ), \n",
    "                (\n",
    "                    'encoder', \n",
    "                    sklearn.preprocessing.OneHotEncoder(categorical_features=categorical_indices)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        hotencoded = clf.fit_transform(X)\n",
    "        if hotencoded.shape[1] > 5000:\n",
    "            data_status[dataset_id] = 'High-dimensional (after one hot encoding.)'\n",
    "            print(dataset.name, 'too high one-hot-encoded dimensionality', hotencoded.shape[1])\n",
    "            continue\n",
    "\n",
    "        print('building decision stump')\n",
    "        clf = sklearn.pipeline.Pipeline(steps=[('imputer', sklearn.preprocessing.Imputer(strategy='median')), \n",
    "                                               ('classifier', sklearn.tree.DecisionTreeClassifier(max_depth=1))])\n",
    "        _ = clf.fit(X, y)\n",
    "        score = clf.score(X, y)\n",
    "        \n",
    "        print('obtaining cv task .. ')\n",
    "        # TODO\n",
    "\n",
    "        max_score_per_dataset[dataset_id] = {\n",
    "            'score': score,\n",
    "            'name': dataset.name\n",
    "        }\n",
    "    except ValueError as e:\n",
    "        data_status[dataset_id] = 'Python ValueError'\n",
    "        print(dataset_id, e)\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        data_status[dataset_id] = 'Python Exception'\n",
    "        print(dataset_id, e)\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    if max_score_per_dataset[dataset_id][\"score\"] == 1.00:\n",
    "        data_status[dataset_id] = 'Too easy (decisionstump on trainset)'\n",
    "        print(\"Dataset \", dataset.name, \"is too easy.\")\n",
    "    \n",
    "results = pd.DataFrame(max_score_per_dataset).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results.sort_values(by='score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 5 Remove datasets for other reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_status[40705] = 'Missing description.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Results\n",
    "Final list of selected datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "final_datasets = [k for k,v in data_status.items() if v=='OK']\n",
    "print('{} datasets selected'.format(len(final_datasets)))\n",
    "{k:v for k,v in data_names.items() if k in final_datasets}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Passed all tests, but not in OpenML100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "openml100_set = set(oml.datasets.list_datasets(tag=\"OpenML100\").keys()) # OpenML100\n",
    "\n",
    "new_datasets = [k for k,v in data_status.items() if v=='OK' and k not in openml100_set]\n",
    "{k:v for k,v in data_names.items() if k in new_datasets}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Datasets tagged with OpenML100 that did not pass all tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_datasets = [k for k,v in data_status.items() if k in openml100_set and v!='OK']\n",
    "{k:v for k,v in data_names.items() if k in new_datasets}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Reasons to exclude datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "v = {}\n",
    "for key, value in sorted(data_status.items()):\n",
    "    v.setdefault(value, []).append(key)\n",
    "{k:str(v) for k,v in v.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Difference to google doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "names_selected = {v for k,v in data_names.items() if k in final_datasets}\n",
    "google_doc = {\n",
    "    \"phoneme\",\"breast-w\",\"Australian\",\"banknote-authentication\",\"eeg-eye-state\",\"electricity\",\n",
    "    \"analcatdata_dmft\",\"higgs\",\"blood-transfusion-service-center\",\"ilpd\",\"steel-plates-fault\",\n",
    "    \"satimage\",\"mfeat-morphological\",\"balance-scale\",\"credit-a\",\"cmc\",\"diabetes\",\"wilt\",\"MagicTelescope\",\n",
    "    \"vowel\",\"pc3\",\"adult\",\"pc4\",\"ada_agnostic\",\"GesturePhaseSegmentationProcessed\",\"PhishingWebsites\",\n",
    "    \"bank-marketing\",\"cardiotocography\",\"climate-model-simulation-crashes\",\"first-order-theorem-proving\",\n",
    "    \"wall-robot-navigation\",\"dresses-sales\",\"sick\",\"waveform-5000\",\"wdbc\",\"car\",\"tic-tac-toe\",\"mfeat-zernike\",\n",
    "    \"segment\",\"connect-4\",\"kc2\",\"jm1\",\"pc1\",\"kc1\",\"qsar-biodeg\",\"eucalyptus\",\"credit-g\",\"pendigits\",\"vehicle\",\n",
    "    \"letter\",\"optdigits\",\"mfeat-fourier\",\"mfeat-karhunen\",\"kr-vs-kp\",\"ozone-level-8hr\",\"sylva_agnostic\",\n",
    "    \"nomao\",\"spambase\",\"splice\",\"mushroom\",\"cylinder-bands\",\"SpeedDating\",\"texture\",\"mfeat-factors\",\"collins\",\n",
    "    \"mnist_784\",\"gina_agnostic\",\"Bioresponse\",\"Internet-Advertisements\",\"semeion\",\"soybean\",\"madelon\",\"har\",\n",
    "    \"isolet\",\"micro-mass\",\"cnae-9\",\"MiceProtein\",\"one-hundred-plants-margin\",\"one-hundred-plants-shape\",\n",
    "    \"one-hundred-plants-texture\",\"mfeat-pixel\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"New ones!\")\n",
    "for d in sorted(names_selected - google_doc):\n",
    "    print('  ', d)\n",
    "print(\"Dropped ones!\")\n",
    "for d in sorted(google_doc - names_selected):\n",
    "    print('  ', d)\n",
    "print('Difference')\n",
    "for d in sorted(google_doc ^ names_selected):\n",
    "    print('  ', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (pimp)",
   "language": "python",
   "name": "pimp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
